{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "test.csv\n",
      "train.csv\n",
      "\n",
      "0.502140359687\n"
     ]
    }
   ],
   "source": [
    "# THIS WAS MY FIRST SCRIPT AND IT HAS SOME FLAWS. \n",
    "# IT'S STILL HERE BECAUSE I GOT FOND OF IT.\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "%pylab inline\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from itertools import combinations\n",
    "from numpy import array,array_equal\n",
    "\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model \n",
    "from sklearn import naive_bayes \n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "\n",
    "from subprocess import check_output\n",
    "print check_output([\"ls\", \"./input\"]).decode(\"utf8\")\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "#def #print_shapes():\n",
    "    #print('Train: {}\\nTest: {}'.format(train_dataset.shape, test_dataset.shape))\n",
    "train_dataset = pd.read_csv('./input/train.csv', index_col='ID')\n",
    "test_dataset = pd.read_csv('./input/test.csv', index_col='ID')\n",
    "\n",
    "#print_shapes()\n",
    "# How many nulls are there in the datasets?\n",
    "nulls_train = (train_dataset.isnull().sum()==1).sum()\n",
    "nulls_test = (test_dataset.isnull().sum()==1).sum()\n",
    "#print('There are {} nulls in TRAIN and {} nulls in TEST dataset.'.format(nulls_train, nulls_test))\n",
    "# Remove constant features\n",
    "\n",
    "def identify_constant_features(dataframe):\n",
    "    count_uniques = dataframe.apply(lambda x: len(x.unique()))\n",
    "    constants = count_uniques[count_uniques == 1].index.tolist()\n",
    "    return constants\n",
    "\n",
    "constant_features_train = set(identify_constant_features(train_dataset))\n",
    "\n",
    "#print('There were {} constant features in TRAIN dataset.'.format(len(constant_features_train)))\n",
    "\n",
    "# Drop the constant features\n",
    "train_dataset.drop(constant_features_train, inplace=True, axis=1)\n",
    "\n",
    "\n",
    "#print_shapes()\n",
    "# Remove equals features\n",
    "\n",
    "def identify_equal_features(dataframe):\n",
    "    features_to_compare = list(combinations(dataframe.columns.tolist(),2))\n",
    "    equal_features = []\n",
    "    for compare in features_to_compare:\n",
    "        is_equal = array_equal(dataframe[compare[0]],dataframe[compare[1]])\n",
    "        if is_equal:\n",
    "            equal_features.append(list(compare))\n",
    "    return equal_features\n",
    "\n",
    "equal_features_train = identify_equal_features(train_dataset)\n",
    "\n",
    "#print('There were {} pairs of equal features in TRAIN dataset.'.format(len(equal_features_train)))\n",
    "\n",
    "# Remove the second feature of each pair.\n",
    "\n",
    "features_to_drop = array(equal_features_train)[:,1] \n",
    "train_dataset.drop(features_to_drop, axis=1, inplace=True)\n",
    "\n",
    "#print_shapes()\n",
    "# Define the variables model.\n",
    "\n",
    "y_name = 'TARGET'\n",
    "feature_names = train_dataset.columns.tolist()\n",
    "feature_names.remove(y_name)\n",
    "\n",
    "X = train_dataset[feature_names]\n",
    "y = train_dataset[y_name]\n",
    "\n",
    "# Save the features selected for later use.\n",
    "pd.Series(feature_names).to_csv('features_selected_step1.csv', index=False)\n",
    "#print('Features selected\\n{}'.format(feature_names))\n",
    "   \n",
    "    \n",
    "# Proportion of classes\n",
    "y.value_counts()/len(y)\n",
    "\n",
    "skf = cv.StratifiedKFold(y, n_folds=3, shuffle=True)\n",
    "score_metric = 'roc_auc'\n",
    "scores = {}\n",
    "\n",
    "def score_model(model):\n",
    "    return cv.cross_val_score(model, X, y, cv=skf, scoring=score_metric)\n",
    "\n",
    "clfxgb = xgb.XGBClassifier()\n",
    "clfxgb = clfxgb.fit(X, y)\n",
    "probxgb = clfxgb.predict(X)\n",
    "# #print 'XGB', np.shape(probxgb)\n",
    "print metrics.roc_auc_score(y, probxgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.83124251,  0.84162387,  0.83580491])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.cross_val_score(xgb.XGBClassifier(), X, y, cv=skf, scoring=score_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
